#!/usr/bin/python


import esclient
import json
import argparse
import sys
import time
import datetime

startTime = int(round(time.time() * 1000))

parser = argparse.ArgumentParser(description="Get ids of index")

parser.add_argument('--url', '-u', required=True, help="The full URL to the ElasticSearch server, including port")
parser.add_argument('--indexes', '-i', nargs='+', help="One or more index names to dump, may also be aliases. If none specified, ALL indexes are dumped.")
parser.add_argument('--gzip', '-z', action="store_true", help="Use gzip to compress the output")
parser.add_argument('--bzip2', '-b', action="store_true", help="Use bzip2 to compress the output")
parser.add_argument('--query', '-q',required=False, help="Get Dump based on the query")
parser.add_argument('--category', '-c',required=False, help="pass category names")

arguments = parser.parse_args()
time_stamp = datetime.datetime.now().strftime('%d-%b-%G-%I%M%p')
arguments.file = "%s_ids_es-%s.gzip" % (arguments.indexes[0],time_stamp)

if not arguments.indexes:
    indexes = ['_all']
else:
    indexes = arguments.indexes

if arguments.bzip2 and arguments.gzip:
    sys.stderr.write("Invalid combination of options: I can write either bzip2 or gzip, not both.\n")
    sys.exit(1)
 
es = esclient.ESClient(arguments.url)

# TODO: check cluster state before continuing

# Open a file to write to, based on the arguments given 
if arguments.bzip2 and arguments.file:
    import bz2
    f = bz2.BZ2File(arguments.file, 'wb')
elif arguments.gzip and arguments.file:
    import gzip
    f = gzip.open(arguments.file, 'wb')
elif arguments.file:
    f = open(arguments.file, "w")
else:
    if arguments.bzip2 or arguments.gzip:
        sys.stderr.write("This tool will not write compressed output to stdout. You can however pipe the output through gzip or bzip2 to compress the data.\n")
        sys.exit(1)
    else:    
        # use stdout as a file
        f = sys.stdout
    
#custom funcation for get query by category
def get_query_by_category( category ):
    if len(category) > 0:
	 return { "query": { "terms": { "category": category.split(",")} },"fields":[] }
    return ""
    
is_category=0
if not arguments.category:
  category = "all"
else:
  category = arguments.category
  query_body = get_query_by_category(category)
  if(len(query_body) > 0):
    is_category=1

if (is_category == 0):
  if not arguments.query:
    query_body = { "query": { "match_all": {} }, "fields": [] }
  else:
    query_body = json.loads(arguments.query)

print "Started find ids of indexes : %s" %(arguments.indexes)
scroll_id = es.scan(query_body = query_body, indexes = indexes)

indexes = set()
total_docs = 0
while True:
    scrollres = es.scroll(scroll_id)
    # get next scroll_id
    scroll_id = scrollres["_scroll_id"]
    
    hits = scrollres["hits"]["hits"]
    if total_docs  == 0:
      print "Total Documents: %s" % (scrollres["hits"]["total"])
      
    num_results = 0
    for hit in scrollres["hits"]["hits"]:
        if hit["_index"] not in indexes:
            indexes.add(hit["_index"])

        f.write(hit["_id"])
        f.write('\n')
        num_results += 1
        total_docs+=1
    sys.stdout.write(".")
    sys.stdout.flush()
    # See if we reached the end of the data
    if num_results == 0:
        break
# TODO
# write_mappings(indexes)
endTime = int(round(time.time() * 1000))

print "\nTotal dumped docs :",total_docs;
print "Elapsed time: %d ms" % (endTime-startTime)

f.close()